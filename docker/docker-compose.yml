version: "3.9"

services:
  zookeeper:
    image: bitnami/zookeeper:3.8.3
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    ports:
      - "2181:2181"

  kafka:
    image: bitnami/kafka:3.5.1
    hostname: kafka
    depends_on:
      - zookeeper
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_LISTENERS=INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:29092
      - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka:9092,EXTERNAL://localhost:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
    ports:
      - "9092:29092"
    volumes:
      - kafka_data:/bitnami/kafka
    healthcheck:
      test:
        [
          "CMD",
          "kafka-topics.sh",
          "--bootstrap-server",
          "localhost:9092",
          "--list",
        ]
      interval: 10s
      timeout: 5s
      retries: 10
  spark:
    image: bitnami/spark:3.5.1
    environment:
      - SPARK_MODE=master
    ports:
      - "8080:8080"
      - "7077:7077"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 6

  spark-worker:
    image: bitnami/spark:3.5.1
    depends_on: [spark]
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077

  processing:
    build:
      context: ../services/processing/spark
    depends_on:
      kafka:
        condition: service_healthy
      spark:
        condition: service_healthy
    ports:
      - "4041:4040"
    command: >
      spark-submit
      --conf spark.ui.port=4040
      --conf spark.driver.bindAddress=0.0.0.0
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1
      --master spark://spark:7077
      /opt/bitnami/spark/app/stream_agg.py

  airflow:
    image: apache/airflow:2.8.0
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW_DATABASE__SYNC=true
    volumes:
      - ./dags:/opt/airflow/dags
    ports:
      - "8081:8080"
    command: >
      bash -c "airflow db init &&
               airflow scheduler & airflow webserver"

  metabase:
    image: metabase/metabase:v0.47.3
    ports:
      - "3000:3000"

  postgres:
    image: postgres:16
    environment:
      POSTGRES_USER: metriq
      POSTGRES_PASSWORD: metriq
      POSTGRES_DB: fleet
    ports:
      - "5432:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data

  spark-thrift:
    image: bitnami/spark:3.5.1
    depends_on: [spark]
    volumes:
      - thrift_ivy:/tmp/ivy
    environment:
      - HOME=/tmp
      - SPARK_SUBMIT_OPTIONS=--conf spark.jars.ivy=/tmp/ivy
    entrypoint: ["/opt/bitnami/spark/sbin/start-thriftserver.sh"]
    command:
      [
        "--master",
        "spark://spark:7077",
        "--hiveconf",
        "hive.server2.thrift.port=10001",
        "--hiveconf",
        "hive.server2.thrift.bind.host=0.0.0.0",
      ]
    ports:
      - "10001:10001"
      - "4042:4040" # UI
    restart: unless-stopped

  dbt:
    image: ghcr.io/dbt-labs/dbt-spark:1.9.latest
    depends_on:
      - spark-thrift
    working_dir: /workspace
    volumes:
      - ./:/workspace
      - ${HOME}/.dbt:/root/.dbt:ro
    command: tail -f /dev/null

  api:
    build:
      context: ../services
      dockerfile: Dockerfile
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092

  init-kafka:
    image: bitnami/kafka:3.5.1
    depends_on:
      kafka:
        condition: service_healthy
    entrypoint: ["/bin/bash", "-c"]
    command: >
      kafka-topics.sh --bootstrap-server kafka:9092
      --create --if-not-exists
      --topic vehicle.telemetry.raw
      --partitions 1 --replication-factor 1

volumes:
  kafka_data:
  airflow_data:
  pg_data:
  thrift_ivy:
