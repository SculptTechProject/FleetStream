# FleetStream ğŸš€

**FleetStream** is an experimental sensorâ€‘toâ€‘dashboard playground. It emulates a fleet of vehicles, captures raw telemetry, crunches it in realâ€‘time with **Apacheâ€¯Spark Structuredâ€¯Streaming**, and exposes the results for further analysis.

---

## âœ¨Â What does it do?

| Layer             | Tooling                                       | Role                                                                                                                    |
| ----------------- | --------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- |
| **Simulation**    | `scripts/simulation_start.sh` (PythonÂ +Â curl) | Fires random telemetry events into Kafka (`vehicle.telemetry.raw`)                                                      |
| **Queue**         | **ApacheÂ KafkaÂ 3.5**                          | Buffers raw messages and receives aggregated streams                                                                    |
| **Processing**    | **ApacheÂ SparkÂ 3.5** (StructuredÂ Streaming)   | Reads from `vehicle.telemetry.raw`, computes stats (avg. speed, fuel level, etc.) and writes to `vehicle.telemetry.agg` |
| **Orchestration** | **ApacheÂ AirflowÂ 2.8**                        | Scheduled batches & housekeeping (e.g. Kafka log compaction, backups)                                                   |
| **Analytics**     | **Metabaseâ€¯0.47**                             | Live dashboards on the aggregated data                                                                                  |

Everything is wrapped in **DockerÂ Compose** â€“ start/stop the whole stack with a single command.

---

## ğŸ—ï¸Â Architecture at a glance

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   HTTP/JSON   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Simulator â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   Kafka    â”‚â—„â”€â”€â”€â”€â”€â”€â–ºâ”‚ Spark  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  (vehicle.*)  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  topic â”‚(stream)â”‚
                        â–²                    â–²     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚                    â”‚         â”‚
                        â”‚        DAGs        â”‚         â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Airflow    â”‚      Metabase
                 â”‚  REST API  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     Dashboards
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš¡Â Quick start

> **Prerequisites:** Docker & DockerÂ ComposeÂ v2 (Linux, macOS or WSLÂ 2).

```bash
# 1. Clone the repository
$ git clone https://github.com/<yourâ€‘handle>/fleetstream.git
$ cd fleetstream/docker

# 2. Build and launch the entire stack (detached)
$ docker compose up --build -d

# 3. (optional) Create topics if Kafka is fresh
$ bin/create_topics.sh        # helper script

# 4. Open the portals
- Spark UI:  http://localhost:8080
- Metabase:  http://localhost:3000
- Airflow:   http://localhost:8081  (login: admin / admin)

# 5. Start the simulator
$ cd ../scripts
$ ./simulation_start.sh        # ~1 msg/s by default

# 6. Peek at the results
$ docker exec -it docker-kafka-1 \
  /opt/bitnami/kafka/bin/kafka-console-consumer.sh \
  --bootstrap-server kafka:9092 \
  --topic vehicle.telemetry.agg --from-beginning | jq '.'
```

> To stop and wipe volumes: `docker compose down -v`.

---

## ğŸ—‚ï¸Â Repository layout

```
.
â”œâ”€ docker/                  # Compose files, Dockerfiles & helpers
â”‚Â Â â”œâ”€ docker-compose.yml
â”‚Â Â â””â”€ services/
â”‚Â Â Â Â Â â”œâ”€ processing/spark/  # Spark job (stream_agg.py)
â”‚Â Â Â Â Â â””â”€ â€¦
â”œâ”€ scripts/                # Simulator & CLI helpers
â”œâ”€ dags/                   # Airflow DAGs
â””â”€ README.md               # This file
```

---

## ğŸ”§Â Configuration tips

The key knobs live in `docker/docker-compose.yml` â€“ adjust partitions, ports or simulation rate there first.

* `KAFKA_CFG_ADVERTISED_LISTENERS` must be `PLAINTEXT://kafka:9092` inside the stack.
* `SPARK_MODE` = `master` / `worker` depending on the container.
* Tweak executor memory in `services/processing/spark/Dockerfile` (add `--executor-memory`).

---

## ğŸš€Â First things to try

Once the containers are up and humming youâ€™ll probably want to *see something* rather than stare at logs.

1. **Metabase firstâ€‘run wizard** (soon)
   * Open [http://localhost:3000](http://localhost:3000)
   * Create the initial admin user.
   * Add a new *PostgreSQL* database connection **only if** youâ€™ve enabled the future Postgres sink (see Roadmap).
   * Click **Skip** on sample data, then **Ask a question â†’ Native query** and point it to the `vehicle.telemetry.agg` topic via the Kafka JDBC connector (already bundled).

2. **Airflow sanity check** (soon)
   * Visit [http://localhost:8081](http://localhost:8081) (credentials: `admin` / `admin`).
   * Enable the bundled example DAG *`fleetstream_daily_housekeeping`* â€“ it just prints the size of each Kafka topic to the logs every hour.
   * Trigger it manually once and watch the task logs populate.

3. **Build your first dashboard**
   * In Metabase, create a new *Question* with `avg(speed_kmh)` grouped by *5â€‘minute bins* and **vehicle\_id**.
   * Save it to a dashboard named *Fleet overview*.

4. **Verify Spark is streaming**
   * Spark UI â†’ **Streaming** tab â†’ confirm the *Input Rate* isnâ€™t flatâ€‘lining.
   * Click on the latest batch to inspect operator metrics.

Feel free to crank the event rate in `simulation_start.sh --rate 10` (10 msgs/s) â€“ Spark will automatically scale partitions.

---

## ğŸ›£ï¸Â Roadmap

| Phase             | Milestone                                                                                        | Why it matters                                   |
| ----------------- | ------------------------------------------------------------------------------------------------ | ------------------------------------------------ |
| **ğŸ”œÂ Shortâ€‘term** | Persist aggregates to **PostgreSQL** and surface them in Metabase via a CDC pipeline (Debezium). | Durable storage & SQL joins with reference data. |
|                   | Bundle **GrafanaÂ +Â Loki** for centralised dashboards and log aggregation.                        | One place for infra + app metrics.               |
|                   | **GitHubÂ Actions** CI/CD: build & push Docker images, run smoke tests.                           | Reproducible builds & early breakage detection.  |
| **ğŸ›«Â Midâ€‘term**   | Ship a **Helm chart** so the stack can be deployed on any Kubernetes cluster.                    | Cloudâ€‘deployable in a single `helm install`.     |
|                   | Add **Prometheus exporters** for Kafka & Spark to enable alerting.                               | Productionâ€‘grade observability.                  |
|                   | Beefâ€‘up the simulator â€“ realistic fault codes, GPS drifts, harsh braking.                        | More interesting analytics scenarios.            |
| **ğŸŒ…Â Longâ€‘term**  | REST gateway for **real OBDâ€‘II / CANâ€‘bus hardware** ingestion.                                   | Bridge from lab to the road.                     |
|                   | Showcase **stateful & windowed joins** (e.g. geofencing alerts) in Spark.                        | Advanced streamâ€‘processing patterns.             |
|                   | Explore **Edge deployment**: miniâ€‘Kafka + SparkÂ Connect on RaspberryÂ Pi.                         | Lowâ€‘latency local analytics.                     |

*Excited to hack on any of these?* Open an issue or send a PR â€“ contributions welcome! ğŸ‘‹

---

## ğŸ“Â License

Released under the MIT License.
Have fun & drive safe â€“ even if itâ€™s only bytes on the road ğŸš—ğŸ’¨
