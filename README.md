# **FleetStream ğŸš€**

**FleetStream** is an experimental sensorâ€‘toâ€‘dashboard playground. It emulates a fleet of vehicles, captures raw telemetry, crunches it in realâ€‘time with **Apacheâ€¯Spark Structuredâ€¯Streaming**, and exposes the results for further analysis.

---

## **âœ¨ What does it do?**

|  **Layer**          |  **Tooling**                                  |  **Role**                                                                                                                  |
| ---------------------- | ------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------- |
|  **Simulation**     |  **FastAPI**(containerised REST service)      |  Fires random telemetry events into Kafka (**vehicle.telemetry.raw**)                                                       |
|  **Queue**          |  **Apache Kafkaâ€¯3.5**                        |  Buffers raw messages and receives aggregated streams                                                                       |
|  **Processing**     |  **Apache Sparkâ€¯3.5**(Structured Streaming)  |  Reads from**vehicle.telemetry.raw**, computes stats (avg. speed, fuel level, etc.) and writes to**vehicle.telemetry.agg**  |
|  **Orchestration**  |  **Apache Airflowâ€¯2.8**                      |  Scheduled batches & housekeeping (e.g. Kafka log compaction, backups)                                                      |
|  **Analytics**      |  **Metabaseâ€¯0.47**                           |  Live dashboards on the aggregated data                                                                                     |

Everything is wrapped in **Docker Compose** â€“ startâ€¯/â€¯stop the whole stack with a single command.

---

## **ğŸ—ï¸ Architecture at a glance**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   HTTP/JSON   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Simulator â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   Kafka    â”‚â—„â”€â”€â”€â”€â”€â”€â–ºâ”‚ Spark  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  (vehicle.*)  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  topic â”‚(stream)â”‚
                        â–²                    â–²     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚                    â”‚         â”‚
                        â”‚        DAGs        â”‚         â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Airflow    â”‚      Metabase
                 â”‚  REST API  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     Dashboards
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## **âš¡ Quick start**

> **Prerequisites:** Docker & Docker Compose v2 (Linux, macOS or WSL 2).

```
# 1. Clone the repository
$ git clone https://github.com/<yourâ€‘handle>/fleetstream.git
$ cd fleetstream/docker

# 2. Build and launch the entire stack (add -d to detach and mute logs)
$ docker compose up --build

# 3. (optional) Create topics if Kafka is fresh
$ bin/create_topics.sh        # helper script

# 4. Open the portals
- Spark UI:  http://localhost:8080
- Metabase:  http://localhost:3000
- Airflow:   http://localhost:8081  (login: admin / admin)

# 5. Start the simulator
$ ../scripts/simulation_start.sh   # ~1 msg/s by default

# 6. Peek at the results. Peek at the results
$ docker exec -it docker-kafka-1 \
  /opt/bitnami/kafka/bin/kafka-console-consumer.sh \
  --bootstrap-server kafka:9092 \
  --topic vehicle.telemetry.agg --from-beginning | jq '.'
```

> To stop and wipe volumes: **docker compose down -v**.

---

## **ğŸ—‚ï¸ Repository layout**

```
.
â”œâ”€ docker/                       # Compose files, infra images & helpers
â”‚   â”œâ”€ docker-compose.yml
â”‚   â”œâ”€ dags/                     # Airflow DAGs (mounted into the scheduler)
â”‚   â”œâ”€ logs/                     # Local log volume mounts
â”‚   â”œâ”€ spark.Dockerfile          # Custom Spark image
â”‚   â””â”€ â€¦                         # Inne pliki konfiguracyjne
â”œâ”€ services/                     # Application-level code & images
â”‚   â”œâ”€ processing/               # Streaming jobs
â”‚   â”‚   â””â”€ spark/stream_agg.py
â”‚   â”œâ”€ consumer.py               # Kafka consumer helper
â”‚   â”œâ”€ main.py                   # FastAPI entry-point
â”‚   â”œâ”€ Dockerfile                # Builds the API container
â”‚   â””â”€ requirements.txt          # Service-specific deps
â”œâ”€ scripts/                      # Bash helpers â€“ start/stop the simulator
â”‚   â”œâ”€ simulation_start.sh
â”‚   â””â”€ simulation_stop.sh
â”œâ”€ README.md                     # Youâ€™re reading it
â”œâ”€ requirements.txt              # Local dev / tooling deps
â””â”€ .gitignore
```

---

## **ğŸ”§ Configuration tips**

The key knobs live in **docker/docker-compose.yml** â€“ adjust partitions, ports or default simulation rate there first.

* KAFKA\_CFG\_ADVERTISED\_LISTENERS** must be **PLAINTEXT://kafka:9092** inside the stack.**
* **SPARK\_MODE** = **master** / **worker** depending on the container.
* **Tweak executor memory in **services/processing/spark/Dockerfile** (add **--executor-memory**).**

---

## **ğŸš€ First things to try**

Once the containers are up and humming youâ€™ll probably want to *see something* rather than stare at logs.

1. **Metabase firstâ€‘run wizard** (soon)
   * Open [http://localhost:3000](http://localhost:3000)
   * Create the initial admin user.
   * Add a new *PostgreSQL* database connection **only if** youâ€™ve enabled the future Postgres sink (see Roadmap).
   * Click **Skip** on sample data, then **Ask a question â†’ Native query** and point it to the **vehicle.telemetry.agg** topic via the Kafka JDBC connector (already bundled).
2. **Airflow sanity check** (soon)
   * Visit [http://localhost:8081](http://localhost:8081) (credentials: **admin** / **admin**).
   * Enable the bundled example DAG *fleetstream\_daily\_housekeeping* â€“ it just prints the size of each Kafka topic to the logs every hour.
   * Trigger it manually once and watch the task logs populate.
3. **Build your first dashboard**
   * In Metabase, create a new *Question* with **avg(speed\_kmh)** grouped by *5â€‘minute bins* and **vehicle\_id**.
   * Save it to a dashboard named â€‹*Fleet overview*â€‹.
4. **Verify Spark is streaming**
   * Spark UI â†’ **Streaming** tab â†’ confirm the *Input Rate* isnâ€™t flatâ€‘lining.
   * Click on the latest batch to inspect operator metrics.

Feel free to crank the event rate with **curl -X POST http://localhost:8000/start-sim -d 'rate\_hz=10'** (10 msgs/s) â€“ Spark will automatically scale partitions.

---

## **ğŸ›£ï¸ Roadmap**

|  **Phase**           |  **Milestone**                                                                                   |  **Why it matters**                              |
| ----------------------- | --------------------------------------------------------------------------------------------------- | --------------------------------------------------- |
|  **ğŸ”œ Shortâ€‘term**  |  Persist aggregates to**PostgreSQL**and surface them in Metabase via a CDC pipeline (Debezium).  |  Durable storage & SQL joins with reference data  |
|                       |  Bundle**Grafana + Loki**for centralised dashboards and log aggregation.                         |  One place for infra + app metrics                |
|                       |  **GitHub Actions**CI/CD: build & push Docker images, run smoke tests.                           |  Reproducible builds & early breakage detection   |
|  **ğŸ›« Midâ€‘term**    |  Ship a**Helm chart**so the stack can be deployed on any Kubernetes cluster.                     |  Cloudâ€‘deployable in a single**helm install**    |
|                       |  Add**Prometheus exporters**for Kafka & Spark to enable alerting.                                |  Productionâ€‘grade observability                  |
|                       |  Beefâ€‘up the simulator â€“ realistic fault codes, GPS drifts, harsh braking.                      |  More interesting analytics scenarios             |
|  **ğŸŒ… Longâ€‘term**   |  **REST gateway for****real OBDâ€‘II / CANâ€‘bus hardware**ingestion.                              |  Bridge from lab to the road                      |
|                       |  Showcase**stateful & windowed joins**(e.g. geofencing alerts) in Spark.                         |  Advanced streamâ€‘processing patterns             |
|                       |  Explore**Edge deployment**: miniâ€‘Kafka + Spark Connect on Raspberry Pi.                        |  Lowâ€‘latency local analytics                     |

*Excited to hack on any of these?* Open an issue or send a PR â€“ contributions welcome! ğŸ‘‹

---

## **ğŸ“ License**

Released under the MIT License.

Have fun & drive safe â€“ even if itâ€™s only bytes on the road ğŸš—ğŸ’¨

